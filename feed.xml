<feed xmlns="http://www.w3.org/2005/Atom"> <id>https://jackd.github.io</id><title>jackd</title><subtitle>Papers, projects and posts about deep learning theory and applications</subtitle> <updated>2025-07-14T21:44:51+10:00</updated> <author> <name>Dominic Jack</name> <uri>https://jackd.github.io</uri> </author><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="https://jackd.github.io" rel="alternate" type="text/html" /> <generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator> <rights> © 2025 Dominic Jack </rights> <icon>/assets/img/favicons/favicon.ico</icon> <logo>/assets/img/favicons/favicon-96x96.png</logo> <entry><title>Negative Gearing and the CGT Discount: A Modern Portfolio Theory Analysis</title><link href="https://jackd.github.io/posts/mpt/" rel="alternate" type="text/html" title="Negative Gearing and the CGT Discount: A Modern Portfolio Theory Analysis" /><published>2025-07-14T11:00:01+10:00</published> <updated>2025-07-14T21:43:13+10:00</updated> <id>https://jackd.github.io/posts/mpt/</id> <content src="https://jackd.github.io/posts/mpt/" /> <author> <name>Dominic Jack</name> </author> <summary> Introduction The capital gains tax (CGT) discount was introduced in Australia in 1999 to simplify and incentivize investment in the share market. More recently, in the midst of a housing crisis, treasury has advised that the discount has a very small affect on home prices - of the order of 1-2% - and the current federal housing minister is on the record stating their policies aim for modest,... </summary> </entry> <entry><title>Retention LLMs: Analysing Algorithms and Alternative Implementations</title><link href="https://jackd.github.io/posts/retention/" rel="alternate" type="text/html" title="Retention LLMs: Analysing Algorithms and Alternative Implementations" /><published>2023-10-01T11:00:01+10:00</published> <updated>2023-10-21T20:05:10+10:00</updated> <id>https://jackd.github.io/posts/retention/</id> <content src="https://jackd.github.io/posts/retention/" /> <author> <name>Dominic Jack</name> </author> <category term="keras" /> <category term="LLM" /> <category term="jax" /> <summary> Retention networks have been making waves in the large language model scene, with big claims about the potential to replace transformers with training parallelism, cheaper inference and good performance. I noticed some similarities with other work on RWKV and fast attention and wanted to see if any of the ideas were transferable. TL;DR I implemented retention networks using keras-nlp. Th... </summary> </entry> <entry><title>Faster LLMs: Improving RWKV with Parallel Cumulative Sums</title><link href="https://jackd.github.io/posts/improving-rwkv/" rel="alternate" type="text/html" title="Faster LLMs: Improving RWKV with Parallel Cumulative Sums" /><published>2023-10-01T11:00:01+10:00</published> <updated>2023-10-01T11:00:01+10:00</updated> <id>https://jackd.github.io/posts/improving-rwkv/</id> <content src="https://jackd.github.io/posts/improving-rwkv/" /> <author> <name>Dominic Jack</name> </author> <category term="Tensorflow" /> <category term="pytorch" /> <category term="jax" /> <category term="keras" /> <category term="LLM" /> <summary> Large language models are all the craze right now. I was keen to learn about keras-nlp - keras’ natural language processing framework - and recent methods, so I decided to implement RWKV, a popular model originally implemented in pytorch that’s fostered a surprisingly large ecosystem of tools and use cases. While doing so certainly gave me a good understanding of keras-nlp and the RWKV model,... </summary> </entry> <entry><title>Simple Fast Attention: Causal Implementation Experiments</title><link href="https://jackd.github.io/posts/simple-fast-attention/" rel="alternate" type="text/html" title="Simple Fast Attention: Causal Implementation Experiments" /><published>2022-11-30T11:00:01+10:00</published> <updated>2022-12-25T10:50:55+10:00</updated> <id>https://jackd.github.io/posts/simple-fast-attention/</id> <content src="https://jackd.github.io/posts/simple-fast-attention/" /> <author> <name>Dominic Jack</name> </author> <category term="Tensorflow" /> <category term="Machine Learning" /> <category term="Linear Algebra" /> <summary> Having looked at google-research’s fast attention tensorflow implementation and corresponding blog post, I was left scratching my head about the causal attention implementation. This post discusses a simpler implementation. TL;DR We provide implementations for computing low-rank causal attention equivalent to that discussed in the performer paper. Compared to the original implementation, ou... </summary> </entry> <entry><title>How the Omicron Wave will be Different</title><link href="https://jackd.github.io/posts/omicrons-different/" rel="alternate" type="text/html" title="How the Omicron Wave will be Different" /><published>2021-12-22T11:00:01+10:00</published> <updated>2021-12-22T11:00:01+10:00</updated> <id>https://jackd.github.io/posts/omicrons-different/</id> <content src="https://jackd.github.io/posts/omicrons-different/" /> <author> <name>Dominic Jack</name> </author> <category term="Modelling" /> <summary> TL;DR Omicron cases are projected to double every 2-3 days until a significant proportion of the population has become infected. This will result in a wave like nothing we have seen in Australia or oversease before. The peak in daily infections will be here sooner than you think, hit harder than you think, and be over faster than you think. During that peak, life will be very different.... </summary> </entry> </feed>
