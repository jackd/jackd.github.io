<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="theme" content="Chirpy v2.6.2"><meta name="generator" content="Jekyll v4.4.1" /><meta property="og:title" content="Micro-benchmarking in TF2" /><meta name="author" content="Dominic Jack" /><meta property="og:locale" content="en_US" /><meta name="description" content="TL;DR: TF2 Benchmarks don’t have to be hard to write. See example at the bottom and/or tfbm." /><meta property="og:description" content="TL;DR: TF2 Benchmarks don’t have to be hard to write. See example at the bottom and/or tfbm." /><link rel="canonical" href="https://jackd.github.io/posts/micro-benchmarks-tf2/" /><meta property="og:url" content="https://jackd.github.io/posts/micro-benchmarks-tf2/" /><meta property="og:site_name" content="jackd" /><meta property="og:image" content="https://jackd.github.io/assets/img/posts/tf2.jpg" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-01-23T11:00:01+10:00" /><meta name="twitter:card" content="summary_large_image" /><meta property="twitter:image" content="https://jackd.github.io/assets/img/posts/tf2.jpg" /><meta property="twitter:title" content="Micro-benchmarking in TF2" /><meta name="twitter:site" content="@dombombau" /><meta name="twitter:creator" content="@Dominic Jack" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"description":"TL;DR: TF2 Benchmarks don’t have to be hard to write. See example at the bottom and/or tfbm.","author":{"@type":"Person","name":"Dominic Jack"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://jackd.github.io/posts/micro-benchmarks-tf2/"},"headline":"Micro-benchmarking in TF2","dateModified":"2021-01-23T11:00:01+10:00","datePublished":"2021-01-23T11:00:01+10:00","url":"https://jackd.github.io/posts/micro-benchmarks-tf2/","@type":"BlogPosting","image":"https://jackd.github.io/assets/img/posts/tf2.jpg","@context":"https://schema.org"}</script><title>Micro-benchmarking in TF2 | jackd</title><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon.png"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon-precomposed.png"><link rel="apple-touch-icon" sizes="57x57" href="/assets/img/favicons/apple-icon-57x57.png"><link rel="apple-touch-icon" sizes="60x60" href="/assets/img/favicons/apple-icon-60x60.png"><link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicons/apple-icon-72x72.png"><link rel="apple-touch-icon" sizes="76x76" href="/assets/img/favicons/apple-icon-76x76.png"><link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicons/apple-icon-114x114.png"><link rel="apple-touch-icon" sizes="120x120" href="/assets/img/favicons/apple-icon-120x120.png"><link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicons/apple-icon-144x144.png"><link rel="apple-touch-icon" sizes="152x152" href="/assets/img/favicons/apple-icon-152x152.png"><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-icon-180x180.png"><link rel="icon" type="image/png" sizes="192x192" href="/assets/img/favicons/android-icon-192x192.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="96x96" href="/assets/img/favicons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/manifest.json"><meta name='msapplication-config' content='/assets/img/favicons/browserconfig.xml'><meta name="msapplication-TileColor" content="#ffffff"><meta name="msapplication-TileImage" content="/assets/img/favicons/ms-icon-144x144.png"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="cdn.jsdelivr.net"><link rel="dns-prefetch" href="cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous"><link rel="preload" href="/assets/css/post.css" as="style"><link rel="stylesheet" href="/assets/css/post.css"><link rel="preload" as="style" href="/assets/css/lib/bootstrap-toc.min.css"><link rel="stylesheet" href="/assets/css/lib/bootstrap-toc.min.css" /> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script async src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script async src="/assets/js/post.min.js"></script> <script defer src="/app.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column"><div id="nav-wrapper"><div id="profile-wrapper" class="d-flex flex-column"><div id="avatar" class="d-flex justify-content-center"> <a href="/" alt="avatar"> <img src="/assets/img/dom-cvpr.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="profile-text mt-3"><div class="site-title"> <a href="/">jackd</a></div><div class="site-subtitle font-italic">Deep Learning Researcher</div></div></div><ul class="nav flex-column"><li class="nav-item d-flex justify-content-center "> <a href="/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item d-flex justify-content-center "> <a href="/tabs/papers/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-scroll ml-xl-3 mr-xl-3 unloaded"></i> <span>PAPERS</span> </a><li class="nav-item d-flex justify-content-center "> <a href="/tabs/projects/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-code ml-xl-3 mr-xl-3 unloaded"></i> <span>PROJECTS</span> </a><li class="nav-item d-flex justify-content-center "> <a href="/tabs/about/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a><li class="nav-item d-flex justify-content-center "> <a href="/tabs/categories/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item d-flex justify-content-center "> <a href="/tabs/tags/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item d-flex justify-content-center "> <a href="/tabs/archives/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a></ul></div><div class="sidebar-bottom d-flex flex-wrap justify-content-around mt-4"> <span id="mode-toggle-wrapper"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.mode != null) { if (this.mode == ModeToggle.DARK_MODE) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.mode != null) { if (self.mode == ModeToggle.DARK_MODE) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span> <span class="icon-border"></span> <a href="https://github.com/jackd" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href="https://twitter.com/dombombau" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['thedomjack','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Posts </a> </span> <span>Micro-benchmarking in TF2</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Micro-benchmarking in TF2</h1><div class="post-meta text-muted d-flex flex-column"><div> Posted <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Sat, Jan 23, 2021, 11:00 AM +1000" > Jan 23, 2021 <i class="unloaded">2021-01-23T11:00:01+10:00</i> </span> by <span class="author"> Dominic Jack </span></div></div><div class="post-content"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7 " data-src="/assets/img/posts/tf2.jpg" class="post-preview-img"><p>TL;DR: TF2 Benchmarks don’t have to be hard to write. See example at the bottom and/or <a href="https://github.com/jackd/tfbm">tfbm</a>.</p><blockquote><p>“Premature optimization is the root of all evil.” – <cite><a href="https://wiki.c2.com/?StructuredProgrammingWithGoToStatements">Donald Knuth</a></cite></p></blockquote><p>This quote is ubiquitous in software circles, and is arguably even more relevant today than it was back in 1974. However, it is all too often cited when trying to justify lazy coding. Indeed, Knuth goes on to say:</p><blockquote><p>“Yet we should not pass up our opportunities in that critical 3%.”</p></blockquote><p>To be clear, benchmarks aren’t optimizations. Writing benchmarks won’t make your code faster any more than writing tests will fix your bugs. Benchmarks are a critical diagnostic tool for <em>identifying</em> that 3% that should not be ignored and justifying suboptimal performance elsewhere. If you agree with Knuth on the above (and most programmers and software engineers I work with do), then benchmarks should be your friend.</p><p>That said, all too often I find myself pouring over repositories with little-to-no benchmarks. In the tensorflow community at least, I put this down to two main reasons:</p><ul><li>outdated tools that require users to work directly with outdated structures like <code class="language-plaintext highlighter-rouge">Graph</code>s and <code class="language-plaintext highlighter-rouge">Session</code>s; and<li>little documentation about how to use them.</ul><p>This post aims to address both of these, by looking at the tools provided by tensorflow 2 and introducing some higher-level interfaces to streamline micro-benchmark creation and improve reporting.</p><h2 id="motivating-example">Motivating Example</h2><p>Let’s consider one of the most simplest non-trivial operations: transpsoed matrix multiplication, <code class="language-plaintext highlighter-rouge">A @ B.T</code>. <a href="https://www.tensorflow.org/api_docs/python/tf/linalg/matmul">tf.linagl.matmul</a> probably does a good job with <code class="language-plaintext highlighter-rouge">transpose_b=True</code>, but maybe we’re better off with <a href="https://www.tensorflow.org/api_docs/python/tf/einsum">tf.einsum</a>. Maybe a manual transpose would be more performance, or we could somehow achieve better performance by unstacking the rows of <code class="language-plaintext highlighter-rouge">B</code> and performing multiple matrix-vector products.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="k">def</span> <span class="nf">matmul_transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">transpose_b</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">matmul_manual_transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)))</span>

<span class="k">def</span> <span class="nf">matmul_einsum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">ij,kj-&gt;ik</span><span class="sh">"</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">matmul_unstack</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="nf">add_n</span><span class="p">([</span><span class="n">tf</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">matvec</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">yi</span><span class="p">)</span> <span class="k">for</span> <span class="n">yi</span> <span class="ow">in</span> <span class="n">tf</span><span class="p">.</span><span class="nf">unstack</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)])</span>
</pre></table></code></div></div><p>How do we compare performance for these different implementations? Micro-benchmarks to the rescue!</p><h2 id="existing-tools">Existing Tools</h2><p>One reason I believe microbenchmarks are so uncommon in the tensorflow research community is a lack of tools for micro-benchmarking, and poor documentation of those that exist. To be clear, there are extensive benchmarks in the tensorflow repository itself - the practice just doesn’t seem to have penetrate the user base at large.</p><p>There are two main tools provided by the tensorflow framework for benchmarking code:</p><ul><li><a href="https://www.tensorflow.org/api_docs/python/tf/test/Benchmark">tf.test.Benchmark</a>; and<li><a href="https://www.tensorflow.org/api_docs/python/tf/profiler">tf.profiler</a> (with optional usage via <a href="https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard">tf.keras.callbacks.TensorBoard</a>).</ul><p>Without a doubt the recent profiler tool is incredibly helpful for understanding resource utilization and identifying inefficiencies or bottlenecks. There’s an amazing guide <a href="https://www.tensorflow.org/guide/profiler">here</a>, and I would strongly encourage anyone building and training models to have at least a modest understanding of how to use it. However, in my experience the profiler is best used for understanding entire model performance rather than comparing between different versions of a function or library. For that, <code class="language-plaintext highlighter-rouge">Benchmark</code>s really shine. However, based on the documentation it’s not entirely clear how they’re supposed to be used. The following is how I originally wrote benchmarking scripts.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">get_args</span><span class="p">(</span><span class="n">i</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">j</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1024</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">((</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)),</span> <span class="n">tf</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">((</span><span class="n">k</span><span class="p">,</span> <span class="n">j</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">benchmark_matmul_impl</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="nc">Graph</span><span class="p">().</span><span class="nf">as_default</span><span class="p">()</span> <span class="k">as</span> <span class="n">graph</span><span class="p">:</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="nf">get_args</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">compat</span><span class="p">.</span><span class="n">v1</span><span class="p">.</span><span class="nc">Session</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="n">graph</span><span class="p">)</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
            <span class="n">bm</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">test</span><span class="p">.</span><span class="nc">Benchmark</span><span class="p">()</span>
            <span class="n">bm_result</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="nf">run_op_benchmark</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">bm_result</span>

<span class="nf">benchmark_matmul_impl</span><span class="p">(</span><span class="n">matmul_transpose</span><span class="p">)</span>
</pre></table></code></div></div><p>This gives us some output printed to screen.</p><pre><code class="language-txt">entry {
  name: "TensorFlowBenchmark.run_op_benchmark"
  iters: 10
  wall_time: 0.0020650625228881836
  extras {
    key: "allocator_maximum_num_bytes_GPU_0_bfc"
    value {
      double_value: 12582912.0
    }
  }
  extras {
    key: "allocator_maximum_num_bytes_gpu_host_bfc"
    value {
      double_value: 8.0
    }
  }
}
</code></pre><p>That’s definitely not the most user-friendly reporting, but we’re on the right track. The information is also contained in the value returned from <code class="language-plaintext highlighter-rouge">run_op_benchmark</code>, so if we want to compare different implementations we can accumulate results and print the results at the end.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre><td class="rouge-code"><pre>
<span class="n">impls</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">matmul_transpose</span><span class="p">,</span> <span class="n">matmul_manual_transpose</span><span class="p">,</span> <span class="n">matmul_einsum</span><span class="p">,</span> <span class="n">matmul_unstack</span>
<span class="p">)</span>
<span class="n">names_and_results</span> <span class="o">=</span> <span class="p">[(</span><span class="n">impl</span><span class="p">.</span><span class="n">__name__</span><span class="p">,</span> <span class="nf">benchmark_matmul_impl</span><span class="p">(</span><span class="n">impl</span><span class="p">))</span> <span class="k">for</span> <span class="n">impl</span> <span class="ow">in</span> <span class="n">impls</span><span class="p">]</span>

<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">names_and_results</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">---</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">name: </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">wall_time: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="sh">'</span><span class="s">wall_time</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">extras</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="sh">'</span><span class="s">extras</span><span class="sh">'</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nf">sorted</span><span class="p">(</span><span class="n">extras</span><span class="p">):</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">extras</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</pre></table></code></div></div><p>Output:</p><pre><code class="language-txt">---
name: matmul_transpose
wall_time: 0.0022330284118652344
allocator_maximum_num_bytes_GPU_0_bfc: 12582912
allocator_maximum_num_bytes_gpu_host_bfc: 8
wall_time_mean: 0.0022828102111816405
wall_time_stdev: 0.000134608013307533
---
name: matmul_manual_transpose
wall_time: 0.0021229982376098633
...
</code></pre><p>Alright, we seem to be getting somewhere, but there are a few things that need addressing.</p><blockquote><p>Do we really need to still use <code class="language-plaintext highlighter-rouge">Graph</code>s and <code class="language-plaintext highlighter-rouge">Session</code>s?</p></blockquote><p>If you’re new to TF, these were key constructs in TF1 and are still operating behind the scenes for much of TF2 functionality, but the tensorflow team is strongly encouraging users to use higher level interfaces like <code class="language-plaintext highlighter-rouge">tf.function</code>.</p><p>I was somewhat surprised this wasn’t updated with TF2. It turns out I’m not the only one to feel this way, and a bit of digging through the tensorflow source code reveals there’s <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/tests/xla_test.py#L259">an implementation</a> in the tensorflow repository that hides the usage of these archaic constructs. Unfortunately it’s not a part of the tensorflow module itself - but that’s nothing a quick <code class="language-plaintext highlighter-rouge">copy-paste</code> can’t fix. It also allows you to customize configuration like the device (cpu vs gpu) and whether or not to use JIT compilation.</p><blockquote><p>Can we have a general-purpose interface like <a href="https://docs.pytest.org/en/stable/">pytest</a> for benchmarks?</p></blockquote><p>It turns out this is already supported with the <code class="language-plaintext highlighter-rouge">tf.test.Benchmark</code> class and <code class="language-plaintext highlighter-rouge">tf.test.main</code>, though it’s entirely undocumented and I’m not sure how it’s supposed to be found. It turns out running a script which calls <code class="language-plaintext highlighter-rouge">tf.test.main</code> along with the command line flag <code class="language-plaintext highlighter-rouge">--benchmarks=.*</code> (or a more specific filter) with run any methods with names starting with <code class="language-plaintext highlighter-rouge">benchmark</code> for <code class="language-plaintext highlighter-rouge">tf.test.Benchmark</code> classes defined in any imported modules.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="k">class</span> <span class="nc">ExampleBenchmark</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">test</span><span class="p">.</span><span class="n">Benchmark</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">benchmark_foo</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="nc">Graph</span><span class="p">().</span><span class="nf">as_default</span><span class="p">():</span>
      <span class="n">out</span> <span class="o">=</span> <span class="nf">foo</span><span class="p">()</span>
      <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">compat</span><span class="p">.</span><span class="n">v1</span><span class="p">.</span><span class="nc">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">run_op_benchmark</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">'</span><span class="s">__main__</span><span class="sh">'</span><span class="p">:</span>
  <span class="n">tf</span><span class="p">.</span><span class="n">test</span><span class="p">.</span><span class="nf">main</span><span class="p">()</span>
</pre></table></code></div></div><div class="language-bash highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>python example_benchmark.py <span class="nt">--benchmarks</span><span class="o">=</span>.<span class="k">*</span>
</pre></table></code></div></div><h2 id="tfbm"><a href="https://github.com/jackd/tfbm">tfbm</a></h2><p>Personally, I feel the above leaves a lot to be desired. There’s a lot of boilerplate, I frequently have to consult APIs and accumulating results across different benchmarks and reporting results is a pain. In the past, this has made me less inclined to write benchmarks, and developing new ops that I haven’t benchmarked makes me feel dirty. I decided to bite the bullet and write a module to make things as straight forward as possible. <a href="https://github.com/jackd/tfbm">tfbm</a> (tensorflow benchmarks) is the result of that work. In particular, it provides:</p><ul><li>a clean decorator-based API that builds on <code class="language-plaintext highlighter-rouge">tf.test.Benchmark</code>;<li>a simple CLI for running benchmarks and reporting results (these work with any <code class="language-plaintext highlighter-rouge">tf.test.Benchmark</code>, not just those using the <code class="language-plaintext highlighter-rouge">tfbm</code> API); and<li>saving results and comparing saved results (e.g. to compare performance across different versions).</ul><p>The following example demonstrates usage. See the <a href="https://github.com/jackd/tfbm">README.md</a> for more details.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
</pre><td class="rouge-code"><pre><span class="sh">"""</span><span class="s">benchmark_matmul.py - benchmarking of different `A @ B.T` implementations.</span><span class="sh">"""</span>
<span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">from</span> <span class="n">tfbm</span> <span class="kn">import</span> <span class="n">Benchmark</span><span class="p">,</span> <span class="n">benchmark</span>

<span class="k">def</span> <span class="nf">get_factors</span><span class="p">(</span><span class="n">i</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">j</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1024</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">((</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)),</span> <span class="n">tf</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">((</span><span class="n">k</span><span class="p">,</span> <span class="n">j</span><span class="p">))</span>

<span class="k">class</span> <span class="nc">MatmulBenchmark</span><span class="p">(</span><span class="n">Benchmark</span><span class="p">):</span>
    <span class="c1"># every benchmark will be repeated for each of these configurations.
</span>    <span class="n">BENCHMARK_SPEC</span> <span class="o">=</span> <span class="p">[</span>
        <span class="nf">benchmark</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span><span class="p">),</span>
        <span class="nf">benchmark</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="sh">"</span><span class="s">gpu</span><span class="sh">"</span><span class="p">),</span>
        <span class="nf">benchmark</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">XL</span><span class="sh">"</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="mi">4096</span><span class="p">,)</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="sh">"</span><span class="s">gpu</span><span class="sh">"</span><span class="p">),</span>
    <span class="p">]</span>

    <span class="c1"># a single @benchmark annotation marks this method to be benchmarked.
</span>    <span class="nd">@benchmark</span>
    <span class="k">def</span> <span class="nf">matmul_transpose</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="o">*</span><span class="nf">get_factors</span><span class="p">(</span><span class="o">*</span><span class="n">sizes</span><span class="p">),</span> <span class="n">transpose_b</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="nd">@benchmark</span>
    <span class="k">def</span> <span class="nf">matmul_manual_transpose</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="o">*</span><span class="nf">get_factors</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">),</span> <span class="n">transpose_b</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="nd">@benchmark</span>
    <span class="k">def</span> <span class="nf">matmul_einsum</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">ij,kj-&gt;ik</span><span class="sh">"</span><span class="p">,</span> <span class="o">*</span><span class="nf">get_factors</span><span class="p">(</span><span class="o">*</span><span class="n">sizes</span><span class="p">))</span>

    <span class="c1"># benchmark annotations can also be used to specify additional configuration
</span>    <span class="c1"># multiple annotations can be used, in which case multiple benchmarks will be run.
</span>    <span class="c1"># `matmul_unstack` will be benchmarked 6 times - one for each
</span>    <span class="c1"># (`BENCHMARK_SPEC`, @benchmark) combination. In the event of conflict,
</span>    <span class="c1"># configurations specified in method decorators will override those from
</span>    <span class="c1"># `BENCHMARK_SPEC`
</span>    <span class="nd">@benchmark</span><span class="p">(</span><span class="n">xla_jit</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="nd">@benchmark</span><span class="p">(</span><span class="n">xla_jit</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">matmul_unstack</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="nf">get_factors</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="nf">add_n</span><span class="p">([</span><span class="n">tf</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">matvec</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">yi</span><span class="p">)</span> <span class="k">for</span> <span class="n">yi</span> <span class="ow">in</span> <span class="n">tf</span><span class="p">.</span><span class="nf">unstack</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)])</span>
</pre></table></code></div></div><div class="language-bash highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>python <span class="nt">-m</span> tfbm benchmark_matmul.py <span class="nt">--group_by</span><span class="o">=</span>device,spec <span class="nt">--style</span><span class="o">=</span>markdown
</pre></table></code></div></div><p>Sample output:</p><p>Results for device=gpu,spec=None Uniform results:</p><div class="table-wrapper"><table><thead><tr><th>run_id<th>cls<th>device<th>iters<tbody><tr><td>NOW<td>Matmul<td>gpu<td>10</table></div><p>Varied results:</p><div class="table-wrapper"><table><thead><tr><th>test<th>wall_time (us)<th>max_mem_GPU_0_bfc (Mb)<th>max_mem_gpu_host_bfc (b)<th>xla_jit<tbody><tr><td>matmul_unstack_xla_gpu<td>296.474<td>8.000<td>49.000<td>True<tr><td>matmul_transpose_gpu<td>1385.57<td>12.0<td>8.000<td>False<tr><td>matmul_manual_transpose_gpu<td>1471.639<td>12.0<td>8.000<td>False<tr><td>matmul_einsum_gpu<td>1499.891<td>12.0<td>8.000<td>False<tr><td>matmul_unstack_gpu<td>51507.950<td>4104.000<td>12.0<td>False</table></div><p>The command line even has colors!</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7 " data-src="/assets/img/posts/tfbm-output.png" alt="Command line output" /></p><p>As a side note, I’m not entirely sure I trust the numbers for <code class="language-plaintext highlighter-rouge">xla_jit</code> runs. The code that actually does the benchmarks is almost identical to that in the main tensorflow repository though - i.e. I doubt it’s down to something introduced by <code class="language-plaintext highlighter-rouge">tfbm</code>.</p><h2 id="conclusion">Conclusion</h2><p>Micro-benchmarks are important for delivering performant code. The existing <code class="language-plaintext highlighter-rouge">tf.test.Benchmark</code> infrastructure provides us with the raw tools to get the job done, and with a little work you can be confident you’re getting the best from your code, and monitor for unintended degredations from code changes. <a href="https://github.com/jackd/tfbm">tfbm</a> provides a high level interface and convenient CLI for comparing implementations and saving runs for comparison across software versions.</p><p>Feel free to post any questions below. Bug reports, feature requests or general feedback also welcome on the <a href="https://github.com/jackd/tfbm">tfbm</a> repository - I’d love to know what you think.</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/tensorflow/'>Tensorflow</a>, <a href='/categories/machine-learning/'>Machine Learning</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/performance/" class="post-tag no-text-decoration" >performance</a> <a href="/tags/benchmarks/" class="post-tag no-text-decoration" >benchmarks</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Micro-benchmarking in TF2 - jackd&url=https://jackd.github.io/posts/micro-benchmarks-tf2/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Micro-benchmarking in TF2 - jackd&u=https://jackd.github.io/posts/micro-benchmarks-tf2/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=Micro-benchmarking in TF2 - jackd&url=https://jackd.github.io/posts/micro-benchmarks-tf2/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/mpt/">Negative Gearing and the CGT Discount: A Modern Portfolio Theory Analysis</a><li><a href="/posts/retention/">Retention LLMs: Analysing Algorithms and Alternative Implementations</a><li><a href="/posts/simple-fast-attention/">Simple Fast Attention: Causal Implementation Experiments</a><li><a href="/posts/doherty-init/">Digging into Doherty: Implications of Initialization</a><li><a href="/posts/generalized-eig-jvp/">Generalized Eigenvalue Problem Derivatives</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/performance/">performance</a> <a class="post-tag" href="/tags/benchmarks/">benchmarks</a> <a class="post-tag" href="/tags/autograd/">autograd</a> <a class="post-tag" href="/tags/covid/">covid</a> <a class="post-tag" href="/tags/deterministic/">deterministic</a> <a class="post-tag" href="/tags/epidemiology/">epidemiology</a> <a class="post-tag" href="/tags/jax/">jax</a> <a class="post-tag" href="/tags/pre-emptible/">pre emptible</a> <a class="post-tag" href="/tags/finance/">finance</a> <a class="post-tag" href="/tags/parallel/">parallel</a></div></div></div><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/simple-fast-attention/"><div class="card-body"> <span class="timeago small" > Nov 30, 2022 <i class="unloaded">2022-11-30T11:00:01+10:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Simple Fast Attention: Causal Implementation Experiments</h3><div class="text-muted small"><p> Having looked at google-research’s fast attention tensorflow implementation and corresponding blog post, I was left scratching my head about the causal attention implementation. This post discuss...</p></div></div></a></div><div class="card"> <a href="/posts/retention/"><div class="card-body"> <span class="timeago small" > Oct 1, 2023 <i class="unloaded">2023-10-01T11:00:01+10:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Retention LLMs: Analysing Algorithms and Alternative Implementations</h3><div class="text-muted small"><p> Retention networks have been making waves in the large language model scene, with big claims about the potential to replace transformers with training parallelism, cheaper inference and good perf...</p></div></div></a></div><div class="card"> <a href="/posts/improving-rwkv/"><div class="card-body"> <span class="timeago small" > Oct 1, 2023 <i class="unloaded">2023-10-01T11:00:01+10:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Faster LLMs: Improving RWKV with Parallel Cumulative Sums</h3><div class="text-muted small"><p> Large language models are all the craze right now. I was keen to learn about keras-nlp - keras’ natural language processing framework - and recent methods, so I decided to implement RWKV, a popul...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/generalized-eig-jvp/" class="btn btn-outline-primary"><p>Generalized Eigenvalue Problem Derivatives</p></a> <a href="/posts/deq/" class="btn btn-outline-primary"><p>Deep Equilibrium Models in Jax</p></a></div><div id="disqus" class="pt-2 pb-2"><p class="font-italic text-center text-muted small"> Comments powered by <a href="https://disqus.com/">Disqus</a>.</p></div><script src="/assets/js/lib/jquery.disqusloader.min.js"></script> <script> var options = { scriptUrl: '//jackd.disqus.com/embed.js', disqusConfig: function() { this.page.url = 'https://jackd.github.io/posts/micro-benchmarks-tf2/'; this.page.identifier = '/posts/micro-benchmarks-tf2/'; } }; $.disqusLoader('#disqus', options); </script></div></div></div><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script> <script type="text/javascript"> const imgs = document.querySelectorAll('#post-wrapper img'); const observer = lozad(imgs); observer.observe(); </script><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2025 <a href="https://twitter.com/dombombau">Dominic Jack</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy/" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-xl-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/performance/">performance</a> <a class="post-tag" href="/tags/benchmarks/">benchmarks</a> <a class="post-tag" href="/tags/autograd/">autograd</a> <a class="post-tag" href="/tags/covid/">covid</a> <a class="post-tag" href="/tags/deterministic/">deterministic</a> <a class="post-tag" href="/tags/epidemiology/">epidemiology</a> <a class="post-tag" href="/tags/jax/">jax</a> <a class="post-tag" href="/tags/pre-emptible/">pre emptible</a> <a class="post-tag" href="/tags/finance/">finance</a> <a class="post-tag" href="/tags/parallel/">parallel</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://jackd.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"><div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>{categories}</div><div><i class="fa fa-tag fa-fw"></i>{tags}</div></div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>' }); </script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); </script>
