<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="theme" content="Chirpy v2.6.2"><meta name="generator" content="Jekyll v4.4.1" /><meta property="og:title" content="Faster LLMs: Improving RWKV with Parallel Cumulative Sums" /><meta name="author" content="Dominic Jack" /><meta property="og:locale" content="en_US" /><meta name="description" content="Papers, projects and posts about deep learning theory and applications" /><meta property="og:description" content="Papers, projects and posts about deep learning theory and applications" /><link rel="canonical" href="https://jackd.github.io/posts/improving-rwkv/" /><meta property="og:url" content="https://jackd.github.io/posts/improving-rwkv/" /><meta property="og:site_name" content="jackd" /><meta property="og:image" content="https://github.com/jackd/keras-rwkv/blob/master/images/benchmark-032.png?raw=true" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2023-10-01T11:00:01+10:00" /><meta name="twitter:card" content="summary_large_image" /><meta property="twitter:image" content="https://github.com/jackd/keras-rwkv/blob/master/images/benchmark-032.png?raw=true" /><meta property="twitter:title" content="Faster LLMs: Improving RWKV with Parallel Cumulative Sums" /><meta name="twitter:site" content="@dombombau" /><meta name="twitter:creator" content="@Dominic Jack" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"description":"Papers, projects and posts about deep learning theory and applications","author":{"@type":"Person","name":"Dominic Jack"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://jackd.github.io/posts/improving-rwkv/"},"headline":"Faster LLMs: Improving RWKV with Parallel Cumulative Sums","dateModified":"2023-10-01T11:00:01+10:00","datePublished":"2023-10-01T11:00:01+10:00","url":"https://jackd.github.io/posts/improving-rwkv/","@type":"BlogPosting","image":"https://github.com/jackd/keras-rwkv/blob/master/images/benchmark-032.png?raw=true","@context":"https://schema.org"}</script><title>Faster LLMs: Improving RWKV with Parallel Cumulative Sums | jackd</title><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon.png"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon-precomposed.png"><link rel="apple-touch-icon" sizes="57x57" href="/assets/img/favicons/apple-icon-57x57.png"><link rel="apple-touch-icon" sizes="60x60" href="/assets/img/favicons/apple-icon-60x60.png"><link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicons/apple-icon-72x72.png"><link rel="apple-touch-icon" sizes="76x76" href="/assets/img/favicons/apple-icon-76x76.png"><link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicons/apple-icon-114x114.png"><link rel="apple-touch-icon" sizes="120x120" href="/assets/img/favicons/apple-icon-120x120.png"><link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicons/apple-icon-144x144.png"><link rel="apple-touch-icon" sizes="152x152" href="/assets/img/favicons/apple-icon-152x152.png"><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-icon-180x180.png"><link rel="icon" type="image/png" sizes="192x192" href="/assets/img/favicons/android-icon-192x192.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="96x96" href="/assets/img/favicons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/manifest.json"><meta name='msapplication-config' content='/assets/img/favicons/browserconfig.xml'><meta name="msapplication-TileColor" content="#ffffff"><meta name="msapplication-TileImage" content="/assets/img/favicons/ms-icon-144x144.png"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="cdn.jsdelivr.net"><link rel="dns-prefetch" href="cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous"><link rel="preload" href="/assets/css/post.css" as="style"><link rel="stylesheet" href="/assets/css/post.css"><link rel="preload" as="style" href="/assets/css/lib/bootstrap-toc.min.css"><link rel="stylesheet" href="/assets/css/lib/bootstrap-toc.min.css" /> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script async src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script async src="/assets/js/post.min.js"></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> <script defer src="/app.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column"><div id="nav-wrapper"><div id="profile-wrapper" class="d-flex flex-column"><div id="avatar" class="d-flex justify-content-center"> <a href="/" alt="avatar"> <img src="/assets/img/dom-cvpr.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="profile-text mt-3"><div class="site-title"> <a href="/">jackd</a></div><div class="site-subtitle font-italic">Deep Learning Researcher</div></div></div><ul class="nav flex-column"><li class="nav-item d-flex justify-content-center "> <a href="/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item d-flex justify-content-center "> <a href="/tabs/papers/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-scroll ml-xl-3 mr-xl-3 unloaded"></i> <span>PAPERS</span> </a><li class="nav-item d-flex justify-content-center "> <a href="/tabs/projects/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-code ml-xl-3 mr-xl-3 unloaded"></i> <span>PROJECTS</span> </a><li class="nav-item d-flex justify-content-center "> <a href="/tabs/about/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a><li class="nav-item d-flex justify-content-center "> <a href="/tabs/categories/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item d-flex justify-content-center "> <a href="/tabs/tags/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item d-flex justify-content-center "> <a href="/tabs/archives/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a></ul></div><div class="sidebar-bottom d-flex flex-wrap justify-content-around mt-4"> <span id="mode-toggle-wrapper"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.mode != null) { if (this.mode == ModeToggle.DARK_MODE) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.mode != null) { if (self.mode == ModeToggle.DARK_MODE) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span> <span class="icon-border"></span> <a href="https://github.com/jackd" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href="https://twitter.com/dombombau" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['thedomjack','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Posts </a> </span> <span>Faster LLMs: Improving RWKV with Parallel Cumulative Sums</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Faster LLMs: Improving RWKV with Parallel Cumulative Sums</h1><div class="post-meta text-muted d-flex flex-column"><div> Posted <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Sun, Oct 1, 2023, 11:00 AM +1000" > Oct 1, 2023 <i class="unloaded">2023-10-01T11:00:01+10:00</i> </span> by <span class="author"> Dominic Jack </span></div></div><div class="post-content"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7 " data-src="https://github.com/jackd/keras-rwkv/blob/master/images/benchmark-032.png?raw=true" class="post-preview-img"> <script> window.MathJax = { tex: { tags: 'ams', inlineMath: [['$', '$'], ['\\(', '\\)']] } }; </script><p>Large language models are all the craze right now. I was keen to learn about <a href="https://keras.io/keras_nlp/">keras-nlp</a> - keras’ natural language processing framework - and recent methods, so I decided to implement <a href="https://arxiv.org/abs/2305.13048">RWKV</a>, a popular model originally implemented in pytorch that’s fostered a surprisingly large ecosystem of tools and use cases. While doing so certainly gave me a good understanding of <code class="language-plaintext highlighter-rouge">keras-nlp</code> and the <code class="language-plaintext highlighter-rouge">RWKV</code> model, it also led to an implementation with potential to be much faster than the original.</p><h2 id="tldr">TL;DR</h2><ul><li>The <code class="language-plaintext highlighter-rouge">WKV</code> implementation critical to RWKV models can be implemented as a cumulative sum;<li>naive implementation of this cumulative sum leads to numerical issues, but these can be resolved with relatively standard tools;<li>the resulting implementation is parallelizable using <em>associative scan</em> / <em>prefix sum</em> implementations that are available in most deep learning frameworks and accelerator libraries (including triton, cuda, tensorflow and jax);<li>microbenchmarks and training times with this implementation show promise, though further experiments with more compute are required to understand if these benefits are present in scaled up environments; and<li>the code is available at <a href="https://github.com/jackd/keras-rwkv">github.com/jackd/keras-rwkv</a></ul><h2 id="rwkv">RWKV</h2><p>First, some background. RWKV (pronounced “RuWaKVuh”) is named after the four key quantities ($R$, $W$, $K$ and $V$) used in the self-attention mechanism. This post focuses on the WKV part, which is given in the <a href="https://arxiv.org/abs/2305.13048">original paper</a> as</p><p>$ z^\prime_t = \frac{\sum_{i=1}^{t-1}\exp(-(t - 1 - i)w + k_i) v_i + \exp(u + k_t) v_t}{\sum_{i=1}^{t-1}\exp(-(t - 1 - i)w + k_i) + \exp(u + k_t)}. $</p><p>Multiplying top and bottom by $\exp((t - 1)w)$ yields</p><p>$ z^\prime_t = \frac{\sum_{i=1}^{t-1} \exp(k_i + i w)v_i + \exp(u - w + k_t + t_w)v_t}{\sum_{i=1}^{t-1} \exp(k_i + i w) + \exp(u - w + k_t + t_w)}. $</p><p>If we let $\tilde{k}_n = k_n + n w$, this simplifies to</p><p>$ z^\prime_t = \frac{\sum_{i=1}^{t-1} \exp(\tilde{k}<em>i)v_i + \exp(u - w + \tilde{k}_t)v_t}{\sum</em>{i=1}^{t-1} \exp(\tilde{k}_i) + \exp(u - w + \tilde{k}_t)}. $</p><p>This can be computed efficiently using a cumulative sum.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">jax.numpy</span> <span class="k">as</span> <span class="n">jnp</span>

<span class="k">def</span> <span class="nf">wkv_numerator</span><span class="p">(</span>
    <span class="n">v</span><span class="p">,</span> <span class="c1"># [T, C]
</span>    <span class="n">k</span><span class="p">,</span> <span class="c1"># [T, C]
</span>    <span class="n">u</span><span class="p">,</span> <span class="c1"># [C]
</span>    <span class="n">w</span><span class="p">,</span> <span class="c1"># [C]
</span><span class="p">):</span>
    <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">v</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">kt</span> <span class="o">=</span> <span class="n">k</span> <span class="o">+</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">k</span><span class="p">.</span><span class="n">dtype</span><span class="p">)[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">w</span>
    <span class="n">accumulation</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">cumsum</span><span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">kt</span><span class="p">)</span> <span class="o">*</span> <span class="n">v</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">offset</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">u</span> <span class="o">-</span> <span class="n">w</span> <span class="o">+</span> <span class="n">kt</span><span class="p">)</span> <span class="o">*</span> <span class="n">v</span>
    <span class="n">numer</span> <span class="o">=</span> <span class="n">accumulation</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">offset</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">((</span><span class="n">v</span><span class="p">[:</span><span class="mi">1</span><span class="p">],</span> <span class="n">numer</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">wkv</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
    <span class="k">return</span> <span class="nf">wkv_numerator</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="o">/</span> <span class="nf">wkv_numerator</span><span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">v</span><span class="p">),</span> <span class="n">k</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
</pre></table></code></div></div><p>There are multiple benefits to this include:</p><ul><li>simplicity: no custom cuda kernels or hand-written backward passes; and<li>parallelism: <code class="language-plaintext highlighter-rouge">cumsum</code> can be parallelized along the <code class="language-plaintext highlighter-rouge">T</code> dimension.</ul><p>The major downside is that evaluating <code class="language-plaintext highlighter-rouge">exp(kt)</code> is numerically infeasible for long time sequences. To resolve this, we introduce an <em>exponentially weighted</em> parameterization.</p><h2 id="exponentially-weighted-parameterization">Exponentially Weighted Parameterization</h2><p>We define an exponentially weighted parameterization of a value $z$ as</p>\[z = \exp(t) v,\]<p>where we assume $t$ and $v$ are both $\mathcal{O}(1)$. Due to the exponential however, the scales of $z$ can vary dramatically. We can add two exponentially weighted values and return the exponentially weighted parameterization without explicitly evaluating either of them,</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">jax.numpy</span> <span class="k">as</span> <span class="n">jnp</span>

<span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">z1</span><span class="p">,</span> <span class="n">z2</span><span class="p">):</span>
    <span class="n">v1</span><span class="p">,</span> <span class="n">t1</span> <span class="o">=</span> <span class="n">z1</span>
    <span class="n">v2</span><span class="p">,</span> <span class="n">t2</span> <span class="o">=</span> <span class="n">z2</span>
    <span class="n">t_out</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">logaddexp</span><span class="p">(</span><span class="n">t1</span><span class="p">,</span> <span class="n">t2</span><span class="p">)</span>
    <span class="n">v_out</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">t1</span> <span class="o">-</span> <span class="n">t_out</span><span class="p">)</span> <span class="o">*</span> <span class="n">v1</span> <span class="o">+</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">t2</span> <span class="o">-</span> <span class="n">t_out</span><span class="p">)</span> <span class="o">*</span> <span class="n">v2</span>
    <span class="k">return</span> <span class="n">v_out</span><span class="p">,</span> <span class="n">t_out</span>
</pre></table></code></div></div><h2 id="exponentially-weighted-wkv">Exponentially Weighted WKV</h2><p>To make out <code class="language-plaintext highlighter-rouge">wkv</code> implementation numerically stable, we simply replace the <code class="language-plaintext highlighter-rouge">cumsum</code> with a version that supports a custom <code class="language-plaintext highlighter-rouge">add</code> operation - <code class="language-plaintext highlighter-rouge">jax.lax.associative_scan</code>. Note the resulting exponentially weighted values have <code class="language-plaintext highlighter-rouge">t</code> values corresponding to the denominator in the original expression, so there’s no need to compute a separate denominator.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">wkv</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">u</span><span class="p">):</span>
    <span class="n">sequence_length</span> <span class="o">=</span> <span class="n">k</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">kt</span> <span class="o">=</span> <span class="n">k</span> <span class="o">+</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">)[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">w</span>
    <span class="n">v_acc</span><span class="p">,</span> <span class="n">t_acc</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">assoociative_scan</span><span class="p">(</span><span class="n">add</span><span class="p">,</span> <span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">kt</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">v_out</span><span class="p">,</span> <span class="n">t_out</span> <span class="o">=</span> <span class="nf">add</span><span class="p">((</span><span class="n">v_acc</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">t_acc</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="p">(</span><span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">u</span> <span class="o">-</span> <span class="n">w</span> <span class="o">+</span> <span class="n">kt</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">((</span><span class="n">v</span><span class="p">[:</span><span class="mi">1</span><span class="p">],</span> <span class="n">v_out</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></table></code></div></div><p>Note that <code class="language-plaintext highlighter-rouge">associative_scan</code> (a.k.a. <code class="language-plaintext highlighter-rouge">prefix_sum</code>) is a fundamental operation in computer science that has been extensively studied. In particular, it is worth noting that work-efficient parallel implementations exist and are available in <code class="language-plaintext highlighter-rouge">cuda</code>, <code class="language-plaintext highlighter-rouge">jax</code>, <code class="language-plaintext highlighter-rouge">triton</code> (currently only nightly) and <code class="language-plaintext highlighter-rouge">tensorflow-probability</code>.</p><h2 id="benchmark-results">Benchmark Results</h2><p>So how does our implementation stack up against the custom CUDA kernel used in the original RWKV model? Well… it’s difficult to say, because I’ve had to make do with thrashing my laptop which can’t really support anything but the smallest implementations in highly unrealistic training scenarios. That said, the results we do have look promising.</p><p>For a small number of channels we are able to get significant speed-up seemingly constant computation time up to a very high number of time steps (it’s probably $\mathcal{O}(\text{log}(T))$, but compared to $\mathcal{O}(T)$ that looks pretty constant). For extremely long sequences we see an uptick in computation time, probably due to core saturation. Below is a plot using an 32-dimensional embedding.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7 " data-src="https://github.com/jackd/keras-rwkv/blob/master/images/benchmark-032.png?raw=true" alt="Microbenchmark computation time with 32 dimensional embeddings" /></p><p>Admittedly standard embeddings are much higher dimension than this. If we increase it to 256, we see the uptick occur earlier in the jax implementation. I can’t explain why the tensorflow implementation remains fast for so long.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7 " data-src="https://github.com/jackd/keras-rwkv/blob/master/images/benchmark-256.png?raw=true" alt="Microbenchmark computation time with 256 dimensional embeddings" /></p><p>So do these micro-benchmark improvements result in meaningful improvements in training speed? Well… again, it’s hard to say for certain because my laptop wasn’t designed to train large language models. I did prepare a <em>very</em> dirty training script that runs the smallest model with a batch size of 2 however, and the results are again promising. The parallel jax implementation trained the fastest, with a 31% reduction in training time compared to the original CUDA implementation.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7 " data-src="https://github.com/jackd/keras-rwkv/blob/master/images/train-times.png?raw=true" alt="Train step times." /></p><p>Having said all that, there are a few major disclaimers that should be made:</p><ul><li>as mentioned previously, these are highly unrealistic training scenarios on highly unrealistic hardware. If anyone wants to provide the necessary compute, I’d be very happy to run more realistic evaluations;<li>all these timings are based on my own keras implementation. The latest version of keras is still very new and it wouldn’t surprise me if tensorflow and jax optimizations have been prioritised over pytorch;<li>I’ve never done much with pytorch. I attempted to use <code class="language-plaintext highlighter-rouge">torch.compile</code> for the above, but there were errors that needed suppressing; and<li>Similarly, I’ve never done much with triton, and I’m not sure if I’m using <code class="language-plaintext highlighter-rouge">associative_scan</code> correctly in that context.</ul><p>As such, treat the results presented above as a proof of concept and demonstration of potential, rather than indicative of performances on a realistic scale.</p><p>That’s all for today. Check out the <a href="https://github.com/jackd/keras-rwkv">repo</a> if you want to have a play around. I’ll write another post about my experience with keras/keras-nlp soon, but until then, happy coding!</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/tensorflow/'>Tensorflow</a>, <a href='/categories/pytorch/'>pytorch</a>, <a href='/categories/jax/'>jax</a>, <a href='/categories/keras/'>keras</a>, <a href='/categories/llm/'>LLM</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/performance/" class="post-tag no-text-decoration" >performance</a> <a href="/tags/benchmarks/" class="post-tag no-text-decoration" >benchmarks</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Faster LLMs: Improving RWKV with Parallel Cumulative Sums - jackd&url=https://jackd.github.io/posts/improving-rwkv/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Faster LLMs: Improving RWKV with Parallel Cumulative Sums - jackd&u=https://jackd.github.io/posts/improving-rwkv/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=Faster LLMs: Improving RWKV with Parallel Cumulative Sums - jackd&url=https://jackd.github.io/posts/improving-rwkv/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/mpt/">Negative Gearing and the CGT Discount: A Modern Portfolio Theory Analysis</a><li><a href="/posts/retention/">Retention LLMs: Analysing Algorithms and Alternative Implementations</a><li><a href="/posts/simple-fast-attention/">Simple Fast Attention: Causal Implementation Experiments</a><li><a href="/posts/doherty-init/">Digging into Doherty: Implications of Initialization</a><li><a href="/posts/generalized-eig-jvp/">Generalized Eigenvalue Problem Derivatives</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/performance/">performance</a> <a class="post-tag" href="/tags/benchmarks/">benchmarks</a> <a class="post-tag" href="/tags/autograd/">autograd</a> <a class="post-tag" href="/tags/covid/">covid</a> <a class="post-tag" href="/tags/deterministic/">deterministic</a> <a class="post-tag" href="/tags/epidemiology/">epidemiology</a> <a class="post-tag" href="/tags/jax/">jax</a> <a class="post-tag" href="/tags/pre-emptible/">pre emptible</a> <a class="post-tag" href="/tags/finance/">finance</a> <a class="post-tag" href="/tags/parallel/">parallel</a></div></div></div><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/retention/"><div class="card-body"> <span class="timeago small" > Oct 1, 2023 <i class="unloaded">2023-10-01T11:00:01+10:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Retention LLMs: Analysing Algorithms and Alternative Implementations</h3><div class="text-muted small"><p> Retention networks have been making waves in the large language model scene, with big claims about the potential to replace transformers with training parallelism, cheaper inference and good perf...</p></div></div></a></div><div class="card"> <a href="/posts/micro-benchmarks-tf2/"><div class="card-body"> <span class="timeago small" > Jan 23, 2021 <i class="unloaded">2021-01-23T11:00:01+10:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Micro-benchmarking in TF2</h3><div class="text-muted small"><p> TL;DR: TF2 Benchmarks don’t have to be hard to write. See example at the bottom and/or tfbm. “Premature optimization is the root of all evil.” – Donald Knuth This quote is ubiquitous in softw...</p></div></div></a></div><div class="card"> <a href="/posts/simple-fast-attention/"><div class="card-body"> <span class="timeago small" > Nov 30, 2022 <i class="unloaded">2022-11-30T11:00:01+10:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Simple Fast Attention: Causal Implementation Experiments</h3><div class="text-muted small"><p> Having looked at google-research’s fast attention tensorflow implementation and corresponding blog post, I was left scratching my head about the causal attention implementation. This post discuss...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/simple-fast-attention/" class="btn btn-outline-primary"><p>Simple Fast Attention: Causal Implementation Experiments</p></a> <a href="/posts/retention/" class="btn btn-outline-primary"><p>Retention LLMs: Analysing Algorithms and Alternative Implementations</p></a></div><div id="disqus" class="pt-2 pb-2"><p class="font-italic text-center text-muted small"> Comments powered by <a href="https://disqus.com/">Disqus</a>.</p></div><script src="/assets/js/lib/jquery.disqusloader.min.js"></script> <script> var options = { scriptUrl: '//jackd.disqus.com/embed.js', disqusConfig: function() { this.page.url = 'https://jackd.github.io/posts/improving-rwkv/'; this.page.identifier = '/posts/improving-rwkv/'; } }; $.disqusLoader('#disqus', options); </script></div></div></div><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script> <script type="text/javascript"> const imgs = document.querySelectorAll('#post-wrapper img'); const observer = lozad(imgs); observer.observe(); </script><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2025 <a href="https://twitter.com/dombombau">Dominic Jack</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy/" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-xl-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/performance/">performance</a> <a class="post-tag" href="/tags/benchmarks/">benchmarks</a> <a class="post-tag" href="/tags/autograd/">autograd</a> <a class="post-tag" href="/tags/covid/">covid</a> <a class="post-tag" href="/tags/deterministic/">deterministic</a> <a class="post-tag" href="/tags/epidemiology/">epidemiology</a> <a class="post-tag" href="/tags/jax/">jax</a> <a class="post-tag" href="/tags/pre-emptible/">pre emptible</a> <a class="post-tag" href="/tags/finance/">finance</a> <a class="post-tag" href="/tags/parallel/">parallel</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://jackd.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"><div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>{categories}</div><div><i class="fa fa-tag fa-fw"></i>{tags}</div></div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>' }); </script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); </script>
